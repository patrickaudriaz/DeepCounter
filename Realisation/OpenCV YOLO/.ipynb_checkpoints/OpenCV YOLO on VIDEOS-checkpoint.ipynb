{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/patrickaudriaz/tb-audriaz/Realisation/OpenCV YOLO',\n",
       " '/home/patrickaudriaz/anaconda3/lib/python37.zip',\n",
       " '/home/patrickaudriaz/anaconda3/lib/python3.7',\n",
       " '/home/patrickaudriaz/anaconda3/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/home/patrickaudriaz/.local/lib/python3.7/site-packages',\n",
       " '/home/patrickaudriaz/anaconda3/lib/python3.7/site-packages',\n",
       " '/home/patrickaudriaz/.local/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/home/patrickaudriaz/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "\n",
    "os.sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_counter = 0\n",
    "out_counter = 0\n",
    "\n",
    "streamIP = \"http://160.98.31.185:8080/stream/video.mjpeg\"\n",
    "\n",
    "input_file = \"../dataset/townlow.mp4\"\n",
    "output_file = \"output/test.avi\"\n",
    "\n",
    "scale = 1 / 255.0\n",
    "\n",
    "# minimum probability to filter weak detections\n",
    "min_confidence = 0.4\n",
    "\n",
    "# threshold when applying non-maxima suppression\n",
    "threshold = 0.5\n",
    "\n",
    "total_frames = 0\n",
    "skip_frames = 1\n",
    "\n",
    "fps_update = 20\n",
    "live_fps = 0\n",
    "\n",
    "# Width of network's input image\n",
    "input_width = 416 \n",
    "# Height of network's input image\n",
    "input_height = 416  \n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "class_file = \"yolo/coco.names\"\n",
    "cfg_file = \"yolo/yolov3.cfg\"\n",
    "weights_file = \"yolo/yolov3.weights\"\n",
    "\n",
    "model_name = \"YOLOv3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640.0 480.0\n"
     ]
    }
   ],
   "source": [
    "# vs = cv2.VideoCapture(streamIP)\n",
    "\n",
    "vs = cv2.VideoCapture(input_file)\n",
    "\n",
    "H = vs.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "W = vs.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "\n",
    "print(W, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "[INFO] ... done !\n"
     ]
    }
   ],
   "source": [
    "# load the COCO class labels:\n",
    "class_names = open(class_file).read().strip().split(\"\\n\")\n",
    "\n",
    "# Load the serialized caffe model from disk:\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "\n",
    "# Give the configuration and weight files for the model and load the \n",
    "# network using them.\n",
    "net = cv2.dnn.readNetFromDarknet(cfg_file, weights_file)\n",
    "\n",
    "print(\"[INFO] ... done !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output layer names:\n",
    "def get_output_layers(net):\n",
    "    layer_names = net.getLayerNames()\n",
    "    layer_names = [layer_names[i[0] - 1]\n",
    "                   for i in net.getUnconnectedOutLayers()]\n",
    "    return layer_names\n",
    "\n",
    "\n",
    "# function to draw bounding box on the detected object with class name and precision\n",
    "def draw_bounding_box(frame, boxes):\n",
    "    if (class_names[class_ids[i]] == \"person\"):\n",
    "        (x, y) = (boxes[i][0], boxes[i][1])\n",
    "        (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "        # Draw label, bounding boxes and confidence:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 128, 255), 2)\n",
    "\n",
    "        text = \"{}: {:.4f}\".format(\n",
    "            class_names[class_ids[i]], confidences[i])\n",
    "        cv2.putText(frame, text, (x, y - 5), font, 0.5, (0, 128, 255), 1, cv2.LINE_AA)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process each frames\n",
    "\n",
    "- https://www.pyimagesearch.com/2018/11/12/yolo-object-detection-with-opencv/\n",
    "- https://github.com/PacktPublishing/Mastering-OpenCV-4-with-Python/blob/master/Chapter12/01-chapter-content/opencv/yolo/object_detection_opencv_yolo_darknet.py\n",
    "- https://www.arunponnusamy.com/yolo-object-detection-opencv-python.html\n",
    "\n",
    "\n",
    "1. load coco names\n",
    "- load YOLO config and weights\n",
    "- load input video\n",
    "- use OpenCV dnn module (readNetFromDarknet)\n",
    "- create __blob__ (img preprocessing) (https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] approx. FPS: 19.10\n"
     ]
    }
   ],
   "source": [
    "# start the frames per second throughput estimator\n",
    "fps = FPS().start()\n",
    "total_fps = FPS().start()\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = vs.read()\n",
    "\n",
    "    # if the frame was not grabbed, then we have reached the end\n",
    "    # of the stream\n",
    "    if not grabbed:\n",
    "        print(\"Done processing !!!\")\n",
    "        break\n",
    "    \n",
    "    # process only every n frames to improve performances\n",
    "    if total_frames % skip_frames == 0:\n",
    "        \n",
    "        # Create the blob with a size of (416, 416), swap red and blue channels\n",
    "        # and also a scale factor of 1/255 = 0,003921568627451:\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            frame, scale, (input_width, input_height), (127.5, 127.5, 127.5), crop=False)\n",
    "\n",
    "        # Feed the input blob to the network, perform inference and get the output:\n",
    "        # Set the input for the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        start = time.time()\n",
    "        layer_outputs = net.forward(get_output_layers(net))\n",
    "        end = time.time()\n",
    "        \n",
    "        # initialize our lists of detected bounding boxes, confidences, and\n",
    "        # class IDs, respectively\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "        # populating these lists with data from our YOLO layer_outputs\n",
    "        # loop over each of the layer outputs\n",
    "        for output in layer_outputs:\n",
    "            # loop over each of the detections\n",
    "            for detection in output:\n",
    "                # Get class ID and confidence of the current detection:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                # Filter out weak predictions:\n",
    "                if confidence > min_confidence:\n",
    "                    # Scale the bounding box coordinates (center, width, height)\n",
    "                    # using the dimensions of the original image:\n",
    "                    box = detection[0:4] * np.array([W, H, W, H])\n",
    "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                    # Calculate the top-left corner of the bounding box:\n",
    "                    x = int(centerX - (width / 2))\n",
    "                    y = int(centerY - (height / 2))\n",
    "\n",
    "                    # Update the information we have for each detection:\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # We can apply non-maxima suppression (eliminate weak and overlapping\n",
    "        # bounding boxes):\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, min_confidence, threshold)\n",
    "\n",
    "        if len(indices) > 0:\n",
    "            for i in indices.flatten():\n",
    "                # Extract the (previously recalculated) bounding box coordinates:\n",
    "                draw_bounding_box(frame, boxes)\n",
    "\n",
    "    # increment the total number of frames processed thus far and\n",
    "    # then update the FPS counter\n",
    "    total_frames = total_frames + 1\n",
    "    fps.update()\n",
    "    \n",
    "    # process only every n frames to improve performances\n",
    "    if total_frames % fps_update == 0:\n",
    "        fps.stop()\n",
    "        live_fps = fps.fps()\n",
    "        # start the frames per second throughput estimator\n",
    "        fps = FPS().start()\n",
    "        \n",
    "    cv2.putText(frame, \"Model : \" + model_name, (0, 15),\n",
    "                font, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Resolution : \" + str(int(W)) + \"x\" +\n",
    "                str(int(H)), (0, 35), font, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"FPS: {:.1f}\".format(live_fps),\n",
    "                (0, 55), font, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Detection : {:.2f} sec\".format(\n",
    "        end - start), (0, 75), font, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    total_fps.update()\n",
    "    \n",
    "    cv2.imshow('RPI', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "total_fps.stop()\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(total_fps.fps()))\n",
    "\n",
    "# release the file pointers\n",
    "# writer.release()\n",
    "vs.release()\n",
    "\n",
    "# close any open windows\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
